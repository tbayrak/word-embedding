{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding_examples_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHKgUsEEjJlZ"
      },
      "source": [
        "**CREATING CORPUS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhCxd68QhwRN"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import wikipedia\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "  text = text.replace(\"â\", \"a\")\n",
        "  # when lowering 'İ' a different character is produced similar to \"i\"\n",
        "  text = text.replace(\"İ\", \"i\").replace(\"I\", \"i\").lower()\n",
        "  text = text.replace(\".\", \" \").replace(\",\", \" \").replace(\"?\", \" \").replace(\"!\", \" \")\n",
        "  \n",
        "  # remove special characters\n",
        "  exception_str = \"çğıöşü\"\n",
        "  text = re.sub('[^a-z ' + exception_str + ']', '', text)\n",
        "\n",
        "  # remove single characters \n",
        "  text = ' '.join([w for w in text.split() if len(w)>1])\n",
        "\n",
        "  # remove multiple space charackters\n",
        "  text = re.sub(r'\\s+', ' ', text, flags=re.I).strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def get_content_from_wikipedia(language, topic_list):\n",
        "\n",
        "  wikipedia.set_lang(language)\n",
        "\n",
        "  content = \"\"\n",
        "\n",
        "  for topic in topic_list:\n",
        "    content += wikipedia.page(topic).content  \n",
        "\n",
        "  sentence_list = sent_tokenize(content)\n",
        "\n",
        "  return sentence_list\n",
        "\n",
        "\n",
        "def get_word_tokenized_corpus(sentence_list):\n",
        "\n",
        "  additional_stop_words = [\"başka\", \"diğer\", \"bir\", \"iki\", \"üç\", \"dört\", \n",
        "                           \"beş\", \"altı\", \"yedi\", \"sekiz\", \"dokuz\", \"on\"]\n",
        "  stop_word_list = nltk.corpus.stopwords.words('turkish') + additional_stop_words\n",
        "\n",
        "  word_tokenized_corpus = []\n",
        "\n",
        "  for sentence in sentence_list:\n",
        "    sentence = preprocess_text(sentence)\n",
        "    if len(sentence) > 0:\n",
        "      word_tokenized_corpus.append([token for token in sentence.split(\" \") if token not in stop_word_list])\n",
        "\n",
        "  return word_tokenized_corpus\n",
        "\n",
        "\n",
        "topic_list = [\"yapay zeka\", \"makine öğrenmesi\", \"derin öğrenme\", \n",
        "              \"genetik algoritma\", \"tavsiye sistemleri\", \"optimizasyon\", \n",
        "              \"teknoloji\", \"yapay sinir ağları\", \n",
        "              \"doğal dil işleme\", \"örüntü tanıma\"]\n",
        "sentence_list = get_content_from_wikipedia('tr', topic_list)\n",
        "word_tokenized_corpus = get_word_tokenized_corpus(sentence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LY5DHCljQjU"
      },
      "source": [
        "**BUILDING EMBEDDING MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5nqz6Zod8ip"
      },
      "source": [
        "embedding_size = 60\n",
        "window_size = 40\n",
        "min_word = 5\n",
        "down_sampling = 1e-2\n",
        "\n",
        "ft_model = FastText(word_tokenized_corpus, size=embedding_size, window=window_size, min_count=min_word, sample=down_sampling, sg=1, iter=100)\n",
        "wv_model = Word2Vec(word_tokenized_corpus, size=embedding_size, window=window_size, min_count=min_word, sample=down_sampling, sg=1, iter=100)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn7fnDzDjtS8"
      },
      "source": [
        "**CREATING TSV FILE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77mt1t9wjx06"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('wv_embeddings.tsv', 'w') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    words = ft_model.wv.vocab.keys()\n",
        "    for word in words:\n",
        "        vector = ft_model.wv.get_vector(word).tolist()\n",
        "        row = [word] + vector\n",
        "        writer.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}