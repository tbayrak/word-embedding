{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding_example__en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHKgUsEEjJlZ"
      },
      "source": [
        "**CREATING CORPUS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhCxd68QhwRN"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models import Word2Vec\n",
        "import wikipedia\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # removing all the special characters\n",
        "    text = re.sub(r'\\W', ' ', str(text))\n",
        "\n",
        "    # removing all the single characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "\n",
        "    # removing multiple spaces with single space\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I).strip()\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    # lemmatization\n",
        "    token_list = text.split()\n",
        "    token_list = [stemmer.lemmatize(word) for word in token_list]\n",
        "    token_list = [word for word in token_list if word not in en_stop]\n",
        "    token_list = [word for word in token_list if len(word) > 3]\n",
        "\n",
        "    preprocessed_text = ' '.join(token_list)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def get_content_from_wikipedia(language, topic_list):\n",
        "\n",
        "  wikipedia.set_lang(language)\n",
        "\n",
        "  content = \"\"\n",
        "\n",
        "  for topic in topic_list:\n",
        "    content += wikipedia.page(topic).content  \n",
        "\n",
        "  sentence_list = sent_tokenize(content)\n",
        "\n",
        "  return sentence_list\n",
        "\n",
        "\n",
        "def get_word_tokenized_corpus(sentence_list):\n",
        "  final_corpus = [preprocess_text(sentence) for sentence in sentence_list if sentence.strip() !='']\n",
        "  word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
        "  word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n",
        "\n",
        "  return word_tokenized_corpus\n",
        "\n",
        "\n",
        "topic_list = [\"artificial intelligence\", \"deep learning\", \"genetic algorithms\", \n",
        "                \"recommendation systems\", \"optimization\", \"technology\", \n",
        "                \"natural language processing\", \"pattern recognition\"]\n",
        "\n",
        "sentence_list = get_content_from_wikipedia('en', topic_list)\n",
        "word_tokenized_corpus = get_word_tokenized_corpus(sentence_list)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LY5DHCljQjU"
      },
      "source": [
        "**BUILDING EMBEDDING MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5nqz6Zod8ip"
      },
      "source": [
        "embedding_size = 60\n",
        "window_size = 40\n",
        "min_word = 5\n",
        "down_sampling = 1e-2\n",
        "\n",
        "ft_model = FastText(word_tokenized_corpus, size=embedding_size, window=window_size, min_count=min_word, sample=down_sampling, sg=1, iter=100)\n",
        "wv_model = Word2Vec(word_tokenized_corpus, size=embedding_size, window=window_size, min_count=min_word, sample=down_sampling, sg=1, iter=100)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn7fnDzDjtS8"
      },
      "source": [
        "**CREATING TSV FILE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77mt1t9wjx06"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('wv_embeddings.tsv', 'w') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    words = ft_model.wv.vocab.keys()\n",
        "    for word in words:\n",
        "        vector = ft_model.wv.get_vector(word).tolist()\n",
        "        row = [word] + vector\n",
        "        writer.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}